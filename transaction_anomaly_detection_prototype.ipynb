{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Anomaly Detection Prototype\n",
    "\n",
    "This notebook demonstrates a prototype for detecting anomalies in financial transaction data using an unsupervised learning approach (Isolation Forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Simulation/Generation\n",
    "\n",
    "We'll generate a synthetic dataset of transactions. Most transactions will be normal, but we'll inject a small percentage of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transaction_data(num_transactions=1500, anomaly_percentage=0.03):\n",
    "    data = []\n",
    "    num_anomalies = int(num_transactions * anomaly_percentage)\n",
    "    transaction_ids = list(range(1, num_transactions + 1))\n",
    "    np.random.shuffle(transaction_ids) # Shuffle IDs to mix anomalies\n",
    "\n",
    "    # Define some securities and their typical price ranges\n",
    "    securities = {\n",
    "        'SEC_A': {'mean_price': 100, 'std_price': 5},\n",
    "        'SEC_B': {'mean_price': 50, 'std_price': 2},\n",
    "        'SEC_C': {'mean_price': 200, 'std_price': 10}\n",
    "    }\n",
    "    security_ids = list(securities.keys())\n",
    "\n",
    "    for i in range(num_transactions):\n",
    "        tx_id = transaction_ids[i]\n",
    "        client_id = np.random.randint(1001, 1101)\n",
    "        sec_id = np.random.choice(security_ids)\n",
    "        buy_sell = np.random.choice([0, 1]) # 0 for Sell, 1 for Buy\n",
    "        timestamp = pd.Timestamp('2023-01-01 09:00:00') + pd.Timedelta(minutes=np.random.randint(0, 8*60*20)) # 20 trading days, 8 hours/day\n",
    "        is_anomaly = 0\n",
    "\n",
    "        # Base normal transaction parameters\n",
    "        base_price = np.random.normal(securities[sec_id]['mean_price'], securities[sec_id]['std_price'])\n",
    "        quantity = np.random.randint(10, 500)\n",
    "\n",
    "        # Inject anomalies\n",
    "        if i < num_anomalies:\n",
    "            is_anomaly = 1\n",
    "            anomaly_type = np.random.choice(['high_amount', 'high_quantity', 'price_dev'])\n",
    "            \n",
    "            if anomaly_type == 'high_amount':\n",
    "                # Price can be normal, but quantity makes amount high\n",
    "                quantity = np.random.randint(2000, 5000) \n",
    "            elif anomaly_type == 'high_quantity':\n",
    "                quantity = np.random.randint(3000, 6000)\n",
    "            elif anomaly_type == 'price_dev':\n",
    "                # Price significantly different from typical for this security\n",
    "                if np.random.rand() > 0.5:\n",
    "                    base_price *= np.random.uniform(1.5, 2.5) # Significantly higher\n",
    "                else:\n",
    "                    base_price *= np.random.uniform(0.3, 0.5) # Significantly lower\n",
    "                quantity = np.random.randint(50, 1000) # Quantity can be normal or slightly elevated\n",
    "        \n",
    "        price = round(max(1.0, base_price), 2) # Ensure price is positive\n",
    "        amount = round(quantity * price, 2)\n",
    "\n",
    "        data.append([\n",
    "            tx_id, client_id, sec_id, buy_sell, quantity, price, timestamp, amount, is_anomaly\n",
    "        ])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\n",
    "        'TransactionID', 'ClientID', 'SecurityID', 'BuySell', 'Quantity', \n",
    "        'Price', 'Timestamp', 'Amount', 'IsAnomaly_GroundTruth'\n",
    "    ])\n",
    "    return df.sort_values(by='Timestamp').reset_index(drop=True)\n",
    "\n",
    "df_transactions = generate_transaction_data(num_transactions=1500, anomaly_percentage=0.04)\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions['IsAnomaly_GroundTruth'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA) & Preprocessing (Brief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic Data Statistics:\")\n",
    "print(df_transactions.describe())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_transactions['Amount'], bins=100, kde=True)\n",
    "plt.title('Distribution of Transaction Amount')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=df_transactions, x='Price', y='Quantity', hue='IsAnomaly_GroundTruth', style='IsAnomaly_GroundTruth', alpha=0.7)\n",
    "plt.title('Price vs. Quantity (Colored by Ground Truth Anomaly)')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Quantity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this prototype, we will select numerical features that are likely to indicate anomalies. `SecurityID` and `BuySell` are categorical. `Timestamp` would require more complex feature engineering (e.g., extracting hour, day of week) which we will omit for this initial prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_model = ['Amount', 'Quantity', 'Price']\n",
    "X = df_transactions[features_for_model].copy()\n",
    "\n",
    "# Scaling features can sometimes help performance for distance-based or variance-based algorithms,\n",
    "# though Isolation Forest is generally robust to feature scaling.\n",
    "# For completeness, let's scale them.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features_for_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementation (Isolation Forest)\n",
    "\n",
    "Isolation Forest is an unsupervised learning algorithm that isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Since anomalies are \"few and different,\" they are more susceptible to isolation and are typically found closer to the root of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate contamination based on our injected anomaly percentage\n",
    "contamination_rate = df_transactions['IsAnomaly_GroundTruth'].value_counts(normalize=True)[1]\n",
    "if contamination_rate == 0: # handle case where no anomalies were injected by chance, or only one class exists\n",
    "    contamination_rate = 'auto'\n",
    "else:\n",
    "    print(f\"Using estimated contamination rate: {contamination_rate:.4f}\")\n",
    "\n",
    "model = IsolationForest(n_estimators=100, \n",
    "                        contamination=contamination_rate, # or 'auto'\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1)\n",
    "\n",
    "model.fit(X_scaled_df)\n",
    "\n",
    "# Predict: -1 for anomalies, 1 for inliers\n",
    "df_transactions['AnomalyScore'] = model.decision_function(X_scaled_df)\n",
    "df_transactions['IsAnomaly_Predicted'] = model.predict(X_scaled_df)\n",
    "\n",
    "# Convert predictions to 0 (normal) / 1 (anomaly)\n",
    "# Original output: 1 for inliers, -1 for outliers\n",
    "df_transactions['IsAnomaly_Predicted'] = df_transactions['IsAnomaly_Predicted'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted Anomaly Counts:\")\n",
    "print(df_transactions['IsAnomaly_Predicted'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Visualization and Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_transactions['AnomalyScore'], bins=50, kde=True)\n",
    "plt.title('Distribution of Anomaly Scores')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "anomalies_df = df_transactions[df_transactions['IsAnomaly_Predicted'] == 1]\n",
    "normals_df = df_transactions[df_transactions['IsAnomaly_Predicted'] == 0]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(normals_df['Amount'], normals_df['Quantity'], c='blue', label='Predicted Normal', alpha=0.5, s=20)\n",
    "plt.scatter(anomalies_df['Amount'], anomalies_df['Quantity'], c='red', label='Predicted Anomaly', alpha=0.7, s=50, marker='x')\n",
    "\n",
    "# Highlight ground truth anomalies for comparison\n",
    "ground_truth_anomalies = df_transactions[df_transactions['IsAnomaly_GroundTruth'] == 1]\n",
    "plt.scatter(ground_truth_anomalies['Amount'], ground_truth_anomalies['Quantity'], \n",
    "            facecolors='none', edgecolors='yellow', s=100, linewidth=2, label='Actual Injected Anomaly')\n",
    "\n",
    "plt.title('Transaction Amount vs. Quantity (Colored by Model Prediction)')\n",
    "plt.xlabel('Transaction Amount')\n",
    "plt.ylabel('Transaction Quantity')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Evaluation against Ground Truth\n",
    "\n",
    "Since we injected anomalies, we have a ground truth. We can use this to evaluate how well our unsupervised model performed. **Note:** In a real-world scenario, true labels are often unavailable or very costly to obtain for unsupervised anomaly detection problems. This step is for validating the prototype's basic capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_transactions['IsAnomaly_GroundTruth']\n",
    "y_pred = df_transactions['IsAnomaly_Predicted']\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Normal', 'Predicted Anomaly'], \n",
    "            yticklabels=['Actual Normal', 'Actual Anomaly'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal (0)', 'Anomaly (1)'] KČ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "**How well did the model identify the injected anomalies?**\n",
    "\n",
    "Based on the confusion matrix and classification report:\n",
    "*   The model identifies a certain number of the injected anomalies (True Positives).\n",
    "*   It also correctly identifies a large number of normal transactions (True Negatives).\n",
    "*   There might be some False Positives (normal transactions flagged as anomalies) and False Negatives (injected anomalies missed by the model).\n",
    "*   The `contamination` parameter in Isolation Forest is crucial. If set too low, it might miss anomalies. If too high, it might flag too many normal points as anomalous. In this prototype, we used the known injection rate, which is an ideal scenario.\n",
    "\n",
    "**Limitations of this simple prototype:**\n",
    "\n",
    "*   **Simulated Data:** The data is synthetic and simplified. Real-world transaction data is far more complex, noisy, and may have more subtle anomalies.\n",
    "*   **Feature Engineering:** We used only a few raw numerical features. More sophisticated feature engineering is typically required, such as:\n",
    "    *   Handling `Timestamp`: Extracting features like hour of day, day of week, time since last transaction for a client, etc.\n",
    "    *   Handling `SecurityID`: One-hot encoding or embedding if the number of securities is large. Anomalies might be specific to certain securities or client behaviors related to securities.\n",
    "    *   Creating interaction features or ratios.\n",
    "    *   Considering client-specific historical behavior (e.g., deviation from a client's own average transaction amount).\n",
    "*   **Model Simplicity:** Isolation Forest is a good baseline, but other algorithms (e.g., Local Outlier Factor (LOF), One-Class SVM, Autoencoders) might perform better or capture different types of anomalies.\n",
    "*   **Static Anomalies:** Our injected anomalies are relatively straightforward (very high amount/quantity, significant price deviation). Real anomalies can be more nuanced.\n",
    "*   **Evaluation:** Evaluation relies on ground truth, which is rare. In practice, anomaly detection evaluation often involves manual review of flagged items by domain experts.\n",
    "*   **Scalability:** For very large datasets, the `sklearn` implementation might need to be run on sampled data, or distributed computing solutions might be necessary.\n",
    "\n",
    "**Next steps for a more robust solution:**\n",
    "\n",
    "*   **Use Real-World Data:** Obtain and analyze actual (anonymized) transaction data.\n",
    "*   **Advanced Feature Engineering:** Incorporate time-based features, categorical features (properly encoded), client-specific aggregates, and interaction terms.\n",
    "*   **Explore Other Algorithms:** Experiment with LOF, One-Class SVM, Gaussian Mixture Models, or deep learning approaches like Autoencoders or LSTMs for sequential transaction data.\n",
    "*   **Dynamic Thresholding:** Anomaly scores often require careful thresholding. This might involve statistical methods or domain expertise to set appropriate alert levels.\n",
    "*   **Feedback Loop:** Implement a mechanism for domain experts to review and label predicted anomalies, which can be used to refine the model (semi-supervised learning) or evaluate its performance over time.\n",
    "*   **Consider Seasonality and Trends:** Financial data often has trends and seasonal patterns that normal behavior might follow. Models should account for this to avoid flagging normal peaks as anomalies.\n",
    "*   **Scalability and Deployment:** Plan for deploying the model in a production environment, including data pipelines, retraining schedules, and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion for Prototype\n",
    "\n",
    "This prototype successfully demonstrated the basic application of the Isolation Forest algorithm for detecting anomalies in simulated transaction data. It highlighted the process of data generation, model training, prediction, and a basic form of evaluation using injected ground truth.\n",
    "\n",
    "The results indicate that even a simple unsupervised model can identify obvious anomalies. However, the discussion section underscores the significant simplifications made and outlines the many considerations and improvements needed to develop a production-ready, robust anomaly detection system for financial transactions. This prototype serves as a foundational step and a proof-of-concept for further development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
